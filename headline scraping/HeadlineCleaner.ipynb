{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement Author</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fentrice Driskell</td>\n",
       "      <td>\"$1 of every $3 (Ron DeSantis) spends comes fr...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Ortt</td>\n",
       "      <td>If New York’s proposed limits on natural gas i...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tony Evers</td>\n",
       "      <td>“Wisconsin is the nation’s top cranberry produ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morgan Luttrell</td>\n",
       "      <td>\"Biden drained America's Strategic Petroleum R...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melissa Agard</td>\n",
       "      <td>\"Historically, our spring elections (including...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>TikTok posts</td>\n",
       "      <td>“As of today, no one has the right to film or ...</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>\"mRNA is not a vaccine\" — it's \"actually an op...</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Video says COVID-19 vaccines are “weapons of m...</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Says Ben Shapiro said on Twitter that his “red...</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Nostradamus predicted that a “feeble man” woul...</td>\n",
       "      <td>pants-fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Statement Author                                          Statement  \\\n",
       "0     Fentrice Driskell  \"$1 of every $3 (Ron DeSantis) spends comes fr...   \n",
       "1           Robert Ortt  If New York’s proposed limits on natural gas i...   \n",
       "2            Tony Evers  “Wisconsin is the nation’s top cranberry produ...   \n",
       "3       Morgan Luttrell  \"Biden drained America's Strategic Petroleum R...   \n",
       "4         Melissa Agard  \"Historically, our spring elections (including...   \n",
       "...                 ...                                                ...   \n",
       "1135       TikTok posts  “As of today, no one has the right to film or ...   \n",
       "1136     Facebook posts  \"mRNA is not a vaccine\" — it's \"actually an op...   \n",
       "1137     Facebook posts  Video says COVID-19 vaccines are “weapons of m...   \n",
       "1138     Facebook posts  Says Ben Shapiro said on Twitter that his “red...   \n",
       "1139     Facebook posts  Nostradamus predicted that a “feeble man” woul...   \n",
       "\n",
       "          Rating  \n",
       "0           true  \n",
       "1           true  \n",
       "2           true  \n",
       "3           true  \n",
       "4           true  \n",
       "...          ...  \n",
       "1135  pants-fire  \n",
       "1136  pants-fire  \n",
       "1137  pants-fire  \n",
       "1138  pants-fire  \n",
       "1139  pants-fire  \n",
       "\n",
       "[1140 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('PolitifactDataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.stem as ns\n",
    "import string\n",
    "import re\n",
    "\n",
    "ps = ns.PorterStemmer()\n",
    "lemma = ns.WordNetLemmatizer()\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "def remove_punctuation(x):\n",
    "    punctuation = string.punctuation\n",
    "    no_punct = \"\".join([word for word in x if word not in punctuation])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    no_sw = [word for word in x if word not in stopwords]\n",
    "    return no_sw\n",
    "\n",
    "#function built to use either stemming or lematization\n",
    "def lemmatize(x):\n",
    "    lemmatized = [lemma.lemmatize(word) for word in x]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "#all of those functions inside one function to keep code clean\n",
    "def clean_data(x):\n",
    "    #tokens = re.sub(\"[^a-zA-Z]\", \" \", x.lower())\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", x)\n",
    "    tokens = essay_v.lower().split()\n",
    "    no_sw = remove_stopwords(tokens)\n",
    "    root = lemmatize(no_sw)\n",
    "    cleaned = ' '.join(root)\n",
    "    return cleaned\n",
    "\n",
    "def clean_tokenize(x):\n",
    "    #tokens = re.sub(\"[^a-zA-Z]\", \" \", x.lower())\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", x)\n",
    "    tokens = essay_v.lower().split()\n",
    "    no_sw = remove_stopwords(tokens)\n",
    "    root = lemmatize(no_sw)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "def count_unique(x):\n",
    "    tokens = x.lower().split()\n",
    "    counts = Counter(tokens)\n",
    "    unique = sum(value == 1 for value in counts.values())\n",
    "    return unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'i got my tooth removed. i dont wanna talk about it!' #example sent to test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def punc_count(x):\n",
    "    return len([c for c in x if c in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness\n",
    "def lexical_richness(x):\n",
    "    lex = LexicalRichness(x)\n",
    "    return lex.mtld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Mr. President, Joe has been identified as the pilot of the 9/11 terrorist attack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_noun(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'NNP' or value[1] == 'NNS' or value[1] == 'NNPS' or value[1] == 'NN' for value in tags)\n",
    "    return count/len(tags)\n",
    "\n",
    "\n",
    "def pos_verb(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'VBP' or value[1] == 'VBN' or value[1] == 'VBG' or value[1] == 'VBD' or value[1] == 'VB' for value in tags)\n",
    "    return count/len(tags)\n",
    "\n",
    "\n",
    "def pos_pronoun(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'PRP' or value[1] == 'PRP$' for value in tags)\n",
    "    return count/len(tags)\n",
    "\n",
    "\n",
    "def pos_interjection(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'UH' for value in tags)\n",
    "    return count/len(tags)\n",
    "\n",
    "\n",
    "def pos_adjective(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'JJ' for value in tags)\n",
    "    return count/len(tags)\n",
    "\n",
    "def pos_determiner(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'DT' for value in tags)\n",
    "    return count/len(tags)\n",
    "\n",
    "def pos_foreign(x):\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    count = sum(value[1] == 'FW' for value in tags)\n",
    "    return count/len(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essay_length  word_count  sentence_count  unique_words  lexical_richness  punctuation_count  spelling_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['Statement'].apply(lambda x : clean_data(x))\n",
    "df['cleaned_tokenized'] = df['Statement'].apply(lambda x : clean_tokenize(x))\n",
    "df['statement_length'] = df['Statement'].apply(lambda x: len(x))\n",
    "df['word_count'] = df['cleaned'].apply(lambda x: len(x))\n",
    "df['sentence_count'] = df['Statement'].apply(lambda x:len(sent_tokenize(x)))\n",
    "df['unique_words'] = df['Statement'].apply(lambda x: count_unique(x))\n",
    "df['lexical_richness'] = df['Statement'].apply(lambda x: lexical_richness(x))\n",
    "df['punctuation_count'] = df['Statement'].apply(lambda x: punc_count(x))\n",
    "df['noun_%'] = df['Statement'].apply(lambda x: pos_noun(x))\n",
    "df['pronoun_%'] = df['Statement'].apply(lambda x: pos_pronoun(x))\n",
    "df['verb_%'] = df['Statement'].apply(lambda x: pos_verb(x))\n",
    "df['adj_%'] = df['Statement'].apply(lambda x: pos_adjective(x))\n",
    "df['determiner_%'] = df['Statement'].apply(lambda x: pos_determiner(x))\n",
    "df['foreign_%'] = df['Statement'].apply(lambda x: pos_foreign(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('PolitifactFeatures.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('PolitifactDatasetFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement Author</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>statement_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>lexical_richness</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>noun_%</th>\n",
       "      <th>pronoun_%</th>\n",
       "      <th>verb_%</th>\n",
       "      <th>adj_%</th>\n",
       "      <th>determiner_%</th>\n",
       "      <th>foreign_%</th>\n",
       "      <th>cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fentrice Driskell</td>\n",
       "      <td>\"$1 of every $3 (Ron DeSantis) spends comes fr...</td>\n",
       "      <td>true</td>\n",
       "      <td>every ron desantis spends come federal government</td>\n",
       "      <td>73</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[every, ron, desantis, spends, come, federal, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Ortt</td>\n",
       "      <td>If New York’s proposed limits on natural gas i...</td>\n",
       "      <td>true</td>\n",
       "      <td>new york proposed limit natural gas building t...</td>\n",
       "      <td>127</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>123.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[new, york, proposed, limit, natural, gas, bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tony Evers</td>\n",
       "      <td>“Wisconsin is the nation’s top cranberry produ...</td>\n",
       "      <td>true</td>\n",
       "      <td>wisconsin nation top cranberry producer fact f...</td>\n",
       "      <td>122</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>101.08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[wisconsin, nation, top, cranberry, producer, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morgan Luttrell</td>\n",
       "      <td>\"Biden drained America's Strategic Petroleum R...</td>\n",
       "      <td>true</td>\n",
       "      <td>biden drained america strategic petroleum rese...</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[biden, drained, america, strategic, petroleum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melissa Agard</td>\n",
       "      <td>\"Historically, our spring elections (including...</td>\n",
       "      <td>true</td>\n",
       "      <td>historically spring election including state s...</td>\n",
       "      <td>117</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[historically, spring, election, including, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>TikTok posts</td>\n",
       "      <td>“As of today, no one has the right to film or ...</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>today one right film photograph mr biden climb...</td>\n",
       "      <td>138</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>102.06</td>\n",
       "      <td>6</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[today, one, right, film, photograph, mr, bide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>\"mRNA is not a vaccine\" — it's \"actually an op...</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>mrna vaccine actually operating system run bil...</td>\n",
       "      <td>105</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>112.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[mrna, vaccine, actually, operating, system, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Video says COVID-19 vaccines are “weapons of m...</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>video say covid vaccine weapon mass destructio...</td>\n",
       "      <td>100</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[video, say, covid, vaccine, weapon, mass, des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Says Ben Shapiro said on Twitter that his “red...</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>say ben shapiro said twitter red pill moment s...</td>\n",
       "      <td>96</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[say, ben, shapiro, said, twitter, red, pill, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>Nostradamus predicted that a “feeble man” woul...</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>nostradamus predicted feeble man would rule we...</td>\n",
       "      <td>98</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>80.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[nostradamus, predicted, feeble, man, would, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Statement Author                                          Statement  \\\n",
       "0     Fentrice Driskell  \"$1 of every $3 (Ron DeSantis) spends comes fr...   \n",
       "1           Robert Ortt  If New York’s proposed limits on natural gas i...   \n",
       "2            Tony Evers  “Wisconsin is the nation’s top cranberry produ...   \n",
       "3       Morgan Luttrell  \"Biden drained America's Strategic Petroleum R...   \n",
       "4         Melissa Agard  \"Historically, our spring elections (including...   \n",
       "...                 ...                                                ...   \n",
       "1135       TikTok posts  “As of today, no one has the right to film or ...   \n",
       "1136     Facebook posts  \"mRNA is not a vaccine\" — it's \"actually an op...   \n",
       "1137     Facebook posts  Video says COVID-19 vaccines are “weapons of m...   \n",
       "1138     Facebook posts  Says Ben Shapiro said on Twitter that his “red...   \n",
       "1139     Facebook posts  Nostradamus predicted that a “feeble man” woul...   \n",
       "\n",
       "          Rating                                            cleaned  \\\n",
       "0           true  every ron desantis spends come federal government   \n",
       "1           true  new york proposed limit natural gas building t...   \n",
       "2           true  wisconsin nation top cranberry producer fact f...   \n",
       "3           true  biden drained america strategic petroleum rese...   \n",
       "4           true  historically spring election including state s...   \n",
       "...          ...                                                ...   \n",
       "1135  pants-fire  today one right film photograph mr biden climb...   \n",
       "1136  pants-fire  mrna vaccine actually operating system run bil...   \n",
       "1137  pants-fire  video say covid vaccine weapon mass destructio...   \n",
       "1138  pants-fire  say ben shapiro said twitter red pill moment s...   \n",
       "1139  pants-fire  nostradamus predicted feeble man would rule we...   \n",
       "\n",
       "      statement_length  word_count  sentence_count  unique_words  \\\n",
       "0                   73          49               1            12   \n",
       "1                  127          98               1            19   \n",
       "2                  122          87               1            18   \n",
       "3                   86          68               1            12   \n",
       "4                  117          85               1            16   \n",
       "...                ...         ...             ...           ...   \n",
       "1135               138          91               3            22   \n",
       "1136               105          66               1            19   \n",
       "1137               100          69               1            16   \n",
       "1138                96          69               1            17   \n",
       "1139                98          69               1            14   \n",
       "\n",
       "      lexical_richness  punctuation_count    noun_%  pronoun_%    verb_%  \\\n",
       "0                10.00                  7  0.157895   0.000000  0.000000   \n",
       "1               123.48                  2  0.407407   0.000000  0.185185   \n",
       "2               101.08                  2  0.428571   0.035714  0.071429   \n",
       "3                12.00                  4  0.312500   0.000000  0.062500   \n",
       "4                16.00                  6  0.272727   0.090909  0.136364   \n",
       "...                ...                ...       ...        ...       ...   \n",
       "1135            102.06                  6  0.454545   0.030303  0.030303   \n",
       "1136            112.00                  7  0.307692   0.038462  0.038462   \n",
       "1137             17.00                  2  0.380952   0.000000  0.142857   \n",
       "1138             17.00                  1  0.450000   0.050000  0.200000   \n",
       "1139             80.92                  1  0.521739   0.000000  0.086957   \n",
       "\n",
       "         adj_%  determiner_%  foreign_%  \\\n",
       "0     0.052632      0.105263        0.0   \n",
       "1     0.111111      0.074074        0.0   \n",
       "2     0.071429      0.071429        0.0   \n",
       "3     0.000000      0.062500        0.0   \n",
       "4     0.000000      0.045455        0.0   \n",
       "...        ...           ...        ...   \n",
       "1135  0.000000      0.090909        0.0   \n",
       "1136  0.000000      0.076923        0.0   \n",
       "1137  0.095238      0.047619        0.0   \n",
       "1138  0.050000      0.000000        0.0   \n",
       "1139  0.043478      0.130435        0.0   \n",
       "\n",
       "                                      cleaned_tokenized  \n",
       "0     [every, ron, desantis, spends, come, federal, ...  \n",
       "1     [new, york, proposed, limit, natural, gas, bui...  \n",
       "2     [wisconsin, nation, top, cranberry, producer, ...  \n",
       "3     [biden, drained, america, strategic, petroleum...  \n",
       "4     [historically, spring, election, including, st...  \n",
       "...                                                 ...  \n",
       "1135  [today, one, right, film, photograph, mr, bide...  \n",
       "1136  [mrna, vaccine, actually, operating, system, r...  \n",
       "1137  [video, say, covid, vaccine, weapon, mass, des...  \n",
       "1138  [say, ben, shapiro, said, twitter, red, pill, ...  \n",
       "1139  [nostradamus, predicted, feeble, man, would, r...  \n",
       "\n",
       "[1140 rows x 17 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2n/b06jflfn76z784mwwtws7gyr0000gp/T/ipykernel_1808/1068174205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statement_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lexical_richness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'punctuation_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'noun_%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pronoun_%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verb_%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adj_%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'determiner_%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'foreign_%'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Statement'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=clean_tokenize)\n",
    "feat_df = df[['statement_length', 'word_count', 'sentence_count', 'unique_words', 'lexical_richness', 'punctuation_count', 'noun_%', 'pronoun_%', 'verb_%', 'adj_%', 'determiner_%', 'foreign_%']]\n",
    "X = pd.concat([pd.DataFrame(vectorizer.fit_transform(df['Statement']).toarray()), feat_df], axis = 1)\n",
    "y = df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>lexical_richness</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>noun_%</th>\n",
       "      <th>pronoun_%</th>\n",
       "      <th>verb_%</th>\n",
       "      <th>adj_%</th>\n",
       "      <th>determiner_%</th>\n",
       "      <th>foreign_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>123.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>101.08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>102.06</td>\n",
       "      <td>6</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>112.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>80.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 3562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  sentence_count  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...             ...   \n",
       "1135  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               3   \n",
       "1136  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1137  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1138  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1139  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "\n",
       "      unique_words  lexical_richness  punctuation_count    noun_%  pronoun_%  \\\n",
       "0               12             10.00                  7  0.157895   0.000000   \n",
       "1               19            123.48                  2  0.407407   0.000000   \n",
       "2               18            101.08                  2  0.428571   0.035714   \n",
       "3               12             12.00                  4  0.312500   0.000000   \n",
       "4               16             16.00                  6  0.272727   0.090909   \n",
       "...            ...               ...                ...       ...        ...   \n",
       "1135            22            102.06                  6  0.454545   0.030303   \n",
       "1136            19            112.00                  7  0.307692   0.038462   \n",
       "1137            16             17.00                  2  0.380952   0.000000   \n",
       "1138            17             17.00                  1  0.450000   0.050000   \n",
       "1139            14             80.92                  1  0.521739   0.000000   \n",
       "\n",
       "        verb_%     adj_%  determiner_%  foreign_%  \n",
       "0     0.000000  0.052632      0.105263        0.0  \n",
       "1     0.185185  0.111111      0.074074        0.0  \n",
       "2     0.071429  0.071429      0.071429        0.0  \n",
       "3     0.062500  0.000000      0.062500        0.0  \n",
       "4     0.136364  0.000000      0.045455        0.0  \n",
       "...        ...       ...           ...        ...  \n",
       "1135  0.030303  0.000000      0.090909        0.0  \n",
       "1136  0.038462  0.000000      0.076923        0.0  \n",
       "1137  0.142857  0.095238      0.047619        0.0  \n",
       "1138  0.200000  0.050000      0.000000        0.0  \n",
       "1139  0.086957  0.043478      0.130435        0.0  \n",
       "\n",
       "[1140 rows x 3562 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2n/b06jflfn76z784mwwtws7gyr0000gp/T/ipykernel_1808/168440983.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'statement_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lexical_richness'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'punctuation_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "num_cols = ['statement_length', 'word_count', 'sentence_count', 'unique_words', 'lexical_richness', 'punctuation_count']\n",
    "\n",
    "ct = ColumnTransformer([('standard_scaler', StandardScaler(), num_cols)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score\n",
    "\n",
    "clfXGB_nocv = Pipeline(steps = [('preprocessor', ct), ('XGBoost', XGBClassifier())])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6ec5c042ddf83e1a0c3f39fbc3e80edf14e579d2f804f9b839dc9985dcd105"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
