{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Statement Author</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>statement_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>lexical_richness</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>noun_%</th>\n",
       "      <th>pronoun_%</th>\n",
       "      <th>verb_%</th>\n",
       "      <th>adj_%</th>\n",
       "      <th>determiner_%</th>\n",
       "      <th>foreign_%</th>\n",
       "      <th>cleaned_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fentrice Driskell</td>\n",
       "      <td>\"$1 of every $3 (Ron DeSantis) spends comes fr...</td>\n",
       "      <td>true</td>\n",
       "      <td>every ron desantis spends come federal government</td>\n",
       "      <td>73</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['every', 'ron', 'desantis', 'spends', 'come',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Robert Ortt</td>\n",
       "      <td>If New York’s proposed limits on natural gas i...</td>\n",
       "      <td>true</td>\n",
       "      <td>new york proposed limit natural gas building t...</td>\n",
       "      <td>127</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>123.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['new', 'york', 'proposed', 'limit', 'natural'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Tony Evers</td>\n",
       "      <td>“Wisconsin is the nation’s top cranberry produ...</td>\n",
       "      <td>true</td>\n",
       "      <td>wisconsin nation top cranberry producer fact f...</td>\n",
       "      <td>122</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>101.08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['wisconsin', 'nation', 'top', 'cranberry', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Morgan Luttrell</td>\n",
       "      <td>\"Biden drained America's Strategic Petroleum R...</td>\n",
       "      <td>true</td>\n",
       "      <td>biden drained america strategic petroleum rese...</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['biden', 'drained', 'america', 'strategic', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Melissa Agard</td>\n",
       "      <td>\"Historically, our spring elections (including...</td>\n",
       "      <td>true</td>\n",
       "      <td>historically spring election including state s...</td>\n",
       "      <td>117</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['historically', 'spring', 'election', 'includ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Statement Author  \\\n",
       "0           0  Fentrice Driskell   \n",
       "1           1        Robert Ortt   \n",
       "2           2         Tony Evers   \n",
       "3           3    Morgan Luttrell   \n",
       "4           4      Melissa Agard   \n",
       "\n",
       "                                           Statement Rating  \\\n",
       "0  \"$1 of every $3 (Ron DeSantis) spends comes fr...   true   \n",
       "1  If New York’s proposed limits on natural gas i...   true   \n",
       "2  “Wisconsin is the nation’s top cranberry produ...   true   \n",
       "3  \"Biden drained America's Strategic Petroleum R...   true   \n",
       "4  \"Historically, our spring elections (including...   true   \n",
       "\n",
       "                                             cleaned  statement_length  \\\n",
       "0  every ron desantis spends come federal government                73   \n",
       "1  new york proposed limit natural gas building t...               127   \n",
       "2  wisconsin nation top cranberry producer fact f...               122   \n",
       "3  biden drained america strategic petroleum rese...                86   \n",
       "4  historically spring election including state s...               117   \n",
       "\n",
       "   word_count  sentence_count  unique_words  lexical_richness  \\\n",
       "0          49               1            12             10.00   \n",
       "1          98               1            19            123.48   \n",
       "2          87               1            18            101.08   \n",
       "3          68               1            12             12.00   \n",
       "4          85               1            16             16.00   \n",
       "\n",
       "   punctuation_count    noun_%  pronoun_%    verb_%     adj_%  determiner_%  \\\n",
       "0                  7  0.157895   0.000000  0.000000  0.052632      0.105263   \n",
       "1                  2  0.407407   0.000000  0.185185  0.111111      0.074074   \n",
       "2                  2  0.428571   0.035714  0.071429  0.071429      0.071429   \n",
       "3                  4  0.312500   0.000000  0.062500  0.000000      0.062500   \n",
       "4                  6  0.272727   0.090909  0.136364  0.000000      0.045455   \n",
       "\n",
       "   foreign_%                                  cleaned_tokenized  \n",
       "0        0.0  ['every', 'ron', 'desantis', 'spends', 'come',...  \n",
       "1        0.0  ['new', 'york', 'proposed', 'limit', 'natural'...  \n",
       "2        0.0  ['wisconsin', 'nation', 'top', 'cranberry', 'p...  \n",
       "3        0.0  ['biden', 'drained', 'america', 'strategic', '...  \n",
       "4        0.0  ['historically', 'spring', 'election', 'includ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('PolitifactDatasetFeatures.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.stem as ns\n",
    "import string\n",
    "import re\n",
    "\n",
    "ps = ns.PorterStemmer()\n",
    "lemma = ns.WordNetLemmatizer()\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "def remove_punctuation(x):\n",
    "    punctuation = string.punctuation\n",
    "    no_punct = \"\".join([word for word in x if word not in punctuation])\n",
    "    return no_punct\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    no_sw = [word for word in x if word not in stopwords]\n",
    "    return no_sw\n",
    "\n",
    "#function built to use either stemming or lematization\n",
    "def lemmatize(x):\n",
    "    lemmatized = [lemma.lemmatize(word) for word in x]\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "#all of those functions inside one function to keep code clean\n",
    "def clean_data(x):\n",
    "    #tokens = re.sub(\"[^a-zA-Z]\", \" \", x.lower())\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", x)\n",
    "    tokens = essay_v.lower().split()\n",
    "    no_sw = remove_stopwords(tokens)\n",
    "    root = lemmatize(no_sw)\n",
    "    cleaned = ' '.join(root)\n",
    "    return cleaned\n",
    "\n",
    "def clean_tokenize(x):\n",
    "    #tokens = re.sub(\"[^a-zA-Z]\", \" \", x.lower())\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", x)\n",
    "    tokens = essay_v.lower().split()\n",
    "    no_sw = remove_stopwords(tokens)\n",
    "    root = lemmatize(no_sw)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=clean_tokenize)\n",
    "feat_df = df[['Statement Author','statement_length', 'word_count', 'sentence_count', 'unique_words', 'lexical_richness', 'punctuation_count', 'noun_%', 'pronoun_%', 'verb_%', 'adj_%', 'determiner_%', 'foreign_%']]\n",
    "X = pd.concat([pd.DataFrame(vectorizer.fit_transform(df['Statement']).toarray()), feat_df], axis = 1)\n",
    "y = df['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             true\n",
       "1             true\n",
       "2             true\n",
       "3             true\n",
       "4             true\n",
       "           ...    \n",
       "1135    pants-fire\n",
       "1136    pants-fire\n",
       "1137    pants-fire\n",
       "1138    pants-fire\n",
       "1139    pants-fire\n",
       "Name: Rating, Length: 1140, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>lexical_richness</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>noun_%</th>\n",
       "      <th>pronoun_%</th>\n",
       "      <th>verb_%</th>\n",
       "      <th>adj_%</th>\n",
       "      <th>determiner_%</th>\n",
       "      <th>foreign_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>123.48</td>\n",
       "      <td>2</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>101.08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>102.06</td>\n",
       "      <td>6</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>112.00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>80.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 3563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  sentence_count  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...             ...   \n",
       "1135  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               3   \n",
       "1136  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1137  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1138  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "1139  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...               1   \n",
       "\n",
       "      unique_words  lexical_richness  punctuation_count    noun_%  pronoun_%  \\\n",
       "0               12             10.00                  7  0.157895   0.000000   \n",
       "1               19            123.48                  2  0.407407   0.000000   \n",
       "2               18            101.08                  2  0.428571   0.035714   \n",
       "3               12             12.00                  4  0.312500   0.000000   \n",
       "4               16             16.00                  6  0.272727   0.090909   \n",
       "...            ...               ...                ...       ...        ...   \n",
       "1135            22            102.06                  6  0.454545   0.030303   \n",
       "1136            19            112.00                  7  0.307692   0.038462   \n",
       "1137            16             17.00                  2  0.380952   0.000000   \n",
       "1138            17             17.00                  1  0.450000   0.050000   \n",
       "1139            14             80.92                  1  0.521739   0.000000   \n",
       "\n",
       "        verb_%     adj_%  determiner_%  foreign_%  \n",
       "0     0.000000  0.052632      0.105263        0.0  \n",
       "1     0.185185  0.111111      0.074074        0.0  \n",
       "2     0.071429  0.071429      0.071429        0.0  \n",
       "3     0.062500  0.000000      0.062500        0.0  \n",
       "4     0.136364  0.000000      0.045455        0.0  \n",
       "...        ...       ...           ...        ...  \n",
       "1135  0.030303  0.000000      0.090909        0.0  \n",
       "1136  0.038462  0.000000      0.076923        0.0  \n",
       "1137  0.142857  0.095238      0.047619        0.0  \n",
       "1138  0.200000  0.050000      0.000000        0.0  \n",
       "1139  0.086957  0.043478      0.130435        0.0  \n",
       "\n",
       "[1140 rows x 3563 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "num_cols = ['statement_length', 'word_count', 'sentence_count', 'unique_words', 'lexical_richness', 'punctuation_count']\n",
    "bin_cols = ['Statement Author']\n",
    "\n",
    "ct = ColumnTransformer([('standard_scaler', StandardScaler(), num_cols),('label_encoder', OrdinalEncoder(), bin_cols)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1858: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.53984794e-01, -5.72570981e-01, -2.98810325e-01, ...,\n",
       "        -6.67862050e-01,  1.69223774e+00,  1.28000000e+02],\n",
       "       [ 6.78293577e-01,  1.10136082e+00, -2.98810325e-01, ...,\n",
       "         1.88230116e+00, -5.57763452e-01,  3.19000000e+02],\n",
       "       [ 5.64193728e-01,  7.25580215e-01, -2.98810325e-01, ...,\n",
       "         1.37892034e+00, -5.57763452e-01,  3.82000000e+02],\n",
       "       ...,\n",
       "       [ 6.21543916e-02,  1.10666490e-01, -2.98810325e-01, ...,\n",
       "        -5.10555542e-01, -5.57763452e-01,  1.26000000e+02],\n",
       "       [-2.91254878e-02,  1.10666490e-01, -2.98810325e-01, ...,\n",
       "        -5.10555542e-01, -1.00776369e+00,  1.26000000e+02],\n",
       "       [ 1.65144519e-02,  1.10666490e-01, -2.98810325e-01, ...,\n",
       "         9.25877597e-01, -1.00776369e+00,  1.26000000e+02]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8859649122807017"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score\n",
    "\n",
    "clfXGB = Pipeline(steps = [('XGBoost', XGBClassifier())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "clfXGB.fit(X_train, y_train)\n",
    "clfXGB.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "clfSVM = Pipeline(steps = [('SVM', SVC())])\n",
    "#logistic regression\n",
    "clfLOGREG = Pipeline(steps = [('Logistic Regression', LogisticRegression())])\n",
    "#XGBoost\n",
    "clfXGB = Pipeline(steps = [('XGBoost', XGBClassifier())])\n",
    "#Naive Bayes\n",
    "clfNB = Pipeline(steps = [('Naive Bayes', GaussianNB())])\n",
    "#Random forest\n",
    "clfRFC = Pipeline(steps = [('RFC', RandomForestClassifier())])\n",
    "#neural netTODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 done\n",
      "model 2 done\n",
      "model 3 done\n",
      "model 4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 5 done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score\n",
    "models = [clfSVM, clfLOGREG, clfXGB, clfNB, clfRFC]\n",
    "\n",
    "model_scores={}\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    name = list(model.named_steps)[0]\n",
    "    \n",
    "    score = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    model_scores[name] = score, mae, mse, f1\n",
    "    \n",
    "\n",
    "    joblib.dump(model, f'{name}Classifier.joblib')\n",
    "    print(f'model {idx+1} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVM': (0.7149122807017544,\n",
       "  0.2850877192982456,\n",
       "  0.2850877192982456,\n",
       "  0.644808743169399),\n",
       " 'Logistic Regression': (0.7412280701754386,\n",
       "  0.25877192982456143,\n",
       "  0.25877192982456143,\n",
       "  0.7203791469194313),\n",
       " 'XGBoost': (0.8859649122807017,\n",
       "  0.11403508771929824,\n",
       "  0.11403508771929824,\n",
       "  0.8828828828828829),\n",
       " 'Naive Bayes': (0.6403508771929824,\n",
       "  0.35964912280701755,\n",
       "  0.35964912280701755,\n",
       "  0.5684210526315789),\n",
       " 'RFC': (0.8508771929824561,\n",
       "  0.14912280701754385,\n",
       "  0.14912280701754385,\n",
       "  0.8454545454545455)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores\n",
    "#scores in order: accuracy, mean absolute error, mean squared error, f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a51e5855e04901d00966621b96dcfa06fb6a1bfd2a37e82f444787cca980996"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
